{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec98377-c09c-4ce8-9b98-b688a998a2e2",
   "metadata": {},
   "source": [
    "# CDA305 â€” Assignment 7\n",
    "# Title: Naive Bayes Classifier\n",
    "# Student: Rupesh Varma\n",
    "# Roll no. 2312res547 \n",
    "# Date: Nov. 11, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9b0b6df-ccc0-4eff-8f6b-e6349de14766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data.tsv!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# load sst2\n",
    "ds = load_dataset(\"stanfordnlp/sst2\", split=\"train\")\n",
    "\n",
    "# sample 800 examples \n",
    "sampled = ds.shuffle(seed=42).select(range(800))\n",
    "\n",
    "# convert label 0->negative, 1->positive\n",
    "def convert_label(x):\n",
    "    return \"positive\" if x[\"label\"] == 1 else \"negative\"\n",
    "\n",
    "# write data.tsv\n",
    "with open(\"data.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        label = convert_label(item)\n",
    "        text = item[\"sentence\"].replace(\"\\t\", \" \") \n",
    "        f.write(f\"{label}\\t{text}\\n\")\n",
    "\n",
    "print(\"Saved data.tsv!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea89669-2a29-42a6-838b-9960dee858ba",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beb06357-be89-4267-947c-9ef7243685e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_data(path):\n",
    "    texts, labels = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for label, text in reader:\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031d227-c45a-4ab4-bf54-157fcd786dff",
   "metadata": {},
   "source": [
    "# Preprocessing & Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f77c1c25-f38e-48dd-95ad-ba8c9ceeec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r\"[a-z']+\", text)\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = Counter()\n",
    "    for t in texts:\n",
    "        vocab.update(tokenize(t))\n",
    "    return sorted(vocab.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2aa9d-d603-45bc-8e5d-432ec422faf2",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes (Word Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bef834b5-50de-4e0b-9449-20e85f9ba1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, texts, labels):\n",
    "        self.labels = list(set(labels))\n",
    "        self.vocab = build_vocab(texts)\n",
    "        self.word_index = {w:i for i,w in enumerate(self.vocab)}\n",
    "\n",
    "        # word count per class\n",
    "        self.word_counts = {label: np.zeros(len(self.vocab)) for label in self.labels}\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        for text, label in zip(texts, labels):\n",
    "            for w in tokenize(text):\n",
    "                if w in self.word_index:\n",
    "                    idx = self.word_index[w]\n",
    "                    self.word_counts[label][idx] += 1\n",
    "\n",
    "        # total words per class\n",
    "        self.total_words = {c: self.word_counts[c].sum() for c in self.labels}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        log_probs = {}\n",
    "\n",
    "        for c in self.labels:\n",
    "            # prior\n",
    "            log_prob = np.log(self.class_counts[c] / sum(self.class_counts.values()))\n",
    "\n",
    "            for w in tokens:\n",
    "                if w in self.word_index:\n",
    "                    idx = self.word_index[w]\n",
    "                    wc = self.word_counts[c][idx]\n",
    "                    num = wc + self.alpha\n",
    "                    den = self.total_words[c] + self.alpha*len(self.vocab)\n",
    "                    log_prob += np.log(num/den)\n",
    "\n",
    "            log_probs[c] = log_prob\n",
    "\n",
    "        return max(log_probs, key=log_probs.get)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        return [self.predict_one(t) for t in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1f72c-b393-4d61-9423-1c81ca23e5ee",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes (Word Presence/Absence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2550f51-a223-420a-8a21-349d1e2d41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, texts, labels):\n",
    "        self.labels = list(set(labels))\n",
    "        self.vocab = build_vocab(texts)\n",
    "        self.word_index = {w:i for i,w in enumerate(self.vocab)}\n",
    "\n",
    "        # presence count\n",
    "        self.present = {c: np.zeros(len(self.vocab)) for c in self.labels}\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        for text, label in zip(texts, labels):\n",
    "            tokens = set(tokenize(text))\n",
    "            for w in tokens:\n",
    "                if w in self.word_index:\n",
    "                    idx = self.word_index[w]\n",
    "                    self.present[label][idx] += 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, text):\n",
    "        tokens = set(tokenize(text))\n",
    "        log_probs = {}\n",
    "\n",
    "        for c in self.labels:\n",
    "            # prior\n",
    "            log_prob = np.log(self.class_counts[c] / sum(self.class_counts.values()))\n",
    "\n",
    "            for w, idx in self.word_index.items():\n",
    "                present_count = self.present[c][idx]\n",
    "\n",
    "                prob = (present_count + self.alpha) / \\\n",
    "                       (self.class_counts[c] + 2*self.alpha)\n",
    "\n",
    "                if w in tokens:\n",
    "                    log_prob += np.log(prob)\n",
    "                else:\n",
    "                    log_prob += np.log(1 - prob)\n",
    "\n",
    "            log_probs[c] = log_prob\n",
    "\n",
    "        return max(log_probs, key=log_probs.get)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        return [self.predict_one(t) for t in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502829e3-fb04-4a68-8c50-1af641bbf6b2",
   "metadata": {},
   "source": [
    "# Cross-Validation (5-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe8b014b-9659-4850-b5be-728334b2b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate(ModelClass, texts, labels, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(texts):\n",
    "        X_train = [texts[i] for i in train_idx]\n",
    "        X_test  = [texts[i] for i in test_idx]\n",
    "        y_train = [labels[i] for i in train_idx]\n",
    "        y_test  = [labels[i] for i in test_idx]\n",
    "\n",
    "        model = ModelClass()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        accs.append(accuracy_score(y_test, preds))\n",
    "        f1s.append(f1_score(y_test, preds, pos_label=\"positive\"))\n",
    "\n",
    "    return accs, f1s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f3ce1-4e18-4022-bdc2-89a1f2f2aa02",
   "metadata": {},
   "source": [
    "# Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b9213e1-def1-43e8-85c9-5a59fd06b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_data(\"data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bf7ef80-4e80-4e35-ab65-5c3be83decbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multinomial Naive Bayes \n",
      "Accuracy: [0.65625, 0.6625, 0.675, 0.675, 0.68125]\n",
      "Average accuracy: 0.67\n",
      "F1-score: [0.7417840375586855, 0.7452830188679245, 0.7346938775510204, 0.7450980392156863, 0.7085714285714285]\n",
      "Average F1: 0.7350860803529491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\" Multinomial Naive Bayes \")\n",
    "acc_m, f1_m = cross_validate(MultinomialNB, texts, labels)\n",
    "print(\"Accuracy:\", acc_m)\n",
    "print(\"Average accuracy:\", sum(acc_m)/5)\n",
    "print(\"F1-score:\", f1_m)\n",
    "print(\"Average F1:\", sum(f1_m)/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79a6b5-3191-4d68-8dcf-91b2125c9384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da9bc9c6-6873-467a-b8a4-088204425dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes \n",
      "Accuracy: [0.60625, 0.63125, 0.58125, 0.68125, 0.60625]\n",
      "Average accuracy: 0.6212500000000001\n",
      "F1-score: [0.7319148936170212, 0.7510548523206751, 0.7148936170212766, 0.7866108786610879, 0.7174887892376681]\n",
      "Average F1: 0.7403926061715458\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Bernoulli Naive Bayes \")\n",
    "acc_b, f1_b = cross_validate(BernoulliNB, texts, labels)\n",
    "print(\"Accuracy:\", acc_b)\n",
    "print(\"Average accuracy:\", sum(acc_b)/5)\n",
    "print(\"F1-score:\", f1_b)\n",
    "print(\"Average F1:\", sum(f1_b)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27dfd1-b1ec-4477-8372-01936bbb108c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871dacb6-03d8-4cea-96d5-bd1f417583bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403a4d98-47f8-4c7c-819e-0afd8470b257",
   "metadata": {},
   "source": [
    "## Cross-Validation Results\r\n",
    "\r\n",
    "We performed **5-fold cross-validation** for both Multinomial Naive Bayes and Bernoulli Naive Bayes models. For each model, the accuracy and F1-score were calculated across all 5 folds.\r\n",
    "\r\n",
    "### Multinomial Naive Bayes\r\n",
    "- Accuracy (5 folds): 0.656, 0.662, 0.675, 0.675, 0.681  \r\n",
    "- **Average Accuracy:** 0.67  \r\n",
    "- F1-score (5 folds): 0.741, 0.745, 0.734, 0.745, 0.708  \r\n",
    "- **Average F1-score:** 0.735  \r\n",
    "\r\n",
    "### Bernoulli Naive Bayes\r\n",
    "- Accuracy (5 folds): 0.606, 0.631, 0.581, 0.681, 0.606  \r\n",
    "- **Average Accuracy:** 0.621  \r\n",
    "- F1-score (5 folds): 0.731, 0.751, 0.714, 0.786, 0.717  \r\n",
    "- **Average F1-score:** 0.740  \r\n",
    "\r\n",
    "### Summary\r\n",
    "The **Multinomial Naive Bayes** model achieved higher accuracy, while the **Bernoulli Naive Bayes** model produced a similar average F1-score. Overall, Multinomial NB performed slightly better for this text sentiment classification task.\r\n",
    "curate result.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce12438-c751-4687-a933-e03628b2d611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
